{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 23498,
          "sourceType": "datasetVersion",
          "datasetId": 310
        }
      ],
      "dockerImageVersionId": 30635,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Credit Card Fraud Detection using RF & XGB",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PratapKodate/Credit-Card-Fraud-Detection/blob/main/Credit_Card_Fraud_Detection_using_RF_%26_XGB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'creditcardfraud:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F310%2F23498%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240903%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240903T061724Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D44e815846732e64aae83091cc4de61027c3bed9ea92088ec4dbe90eca8042fe6d919e403f6513b65c056b3024d6e45004b9522586f8af834d5d164d19eba62783d4637295d3b9eb74518276145815792fe30e3d2ed264b5ebec77fa43599a909cef6294968f36d71b554634b11d6baf03cc4efd35ab6ac19b80047480b3e7e364002e40672bcfba1e1a1b829e87f810def8e520dc62a2dd038260b389bd85a572a303c9a252eadaf4ca259f9236bd00e86d865e48c40f8d414b31bfb2ab2d8b5307c3b31522c03684c7ab41f7681505ab25cb932eadd95858ba2dc0fed9c6e74a51f955be2392161164c5cbe13584fe7f820c41418a6741278159b737ce90c86'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeV3GoL8h_31",
        "outputId": "b5ad5b85-8836-4fef-96e1-4e45cb3a470f"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading creditcardfraud, 69155672 bytes compressed\n",
            "[==================================================] 69155672 bytes downloaded\n",
            "Downloaded and uncompressed: creditcardfraud\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***CREDIT CARD FRAUD DETECTION***"
      ],
      "metadata": {
        "id": "99iOKsRyh_32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.0 About Data\n",
        "- Title: Credit Card Fraud Detection\n",
        "- Dataset: [link](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)\n",
        "\n",
        "  ## Metadata\n",
        "  \n",
        "  `Description:`** The dataset contains transactions made by credit cards in September 2013 by European cardholders.\n",
        "  \n",
        "  **`Data Age:`** Updated (03/05/2021)\n",
        "  \n",
        "  **`License:`** Database: Open Database, Contents: Database Contents"
      ],
      "metadata": {
        "id": "wdJLI0lch_33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.0 Kernel Version Used:\n",
        "- Python 3.11.5"
      ],
      "metadata": {
        "id": "0zuvxOHVh_34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***3.0 Data Preparation:***"
      ],
      "metadata": {
        "id": "NegM5z5Fh_34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.1 Import Libraries\n",
        "\n",
        "Let's start the project by impoprting all the libraries that we will need in this project."
      ],
      "metadata": {
        "id": "iBh-3Q_8h_34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "\n",
        "# 1. to handle the data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# to visualize the dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "# To preprocess the data\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "\n",
        "# machine learning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "#for classification tasks\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestRegressor\n",
        "from xgboost import XGBClassifier\n",
        "#metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:06.699118Z",
          "iopub.execute_input": "2024-01-29T11:44:06.699573Z",
          "iopub.status.idle": "2024-01-29T11:44:06.708423Z",
          "shell.execute_reply.started": "2024-01-29T11:44:06.699539Z",
          "shell.execute_reply": "2024-01-29T11:44:06.707214Z"
        },
        "trusted": true,
        "id": "ujJBwoOFh_34"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2 Data Loading and Exploration | Cleaning"
      ],
      "metadata": {
        "id": "car6m_Lgh_34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 Load the Dataset"
      ],
      "metadata": {
        "id": "wA6dcawvh_35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data from csv file placed locally in our pc\n",
        "df = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:06.710505Z",
          "iopub.execute_input": "2024-01-29T11:44:06.71088Z",
          "iopub.status.idle": "2024-01-29T11:44:10.518133Z",
          "shell.execute_reply.started": "2024-01-29T11:44:06.710836Z",
          "shell.execute_reply": "2024-01-29T11:44:10.516909Z"
        },
        "trusted": true,
        "id": "c_XtytCnh_35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set the option to show maximum columns:"
      ],
      "metadata": {
        "id": "o4913RnHh_35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:10.519765Z",
          "iopub.execute_input": "2024-01-29T11:44:10.520468Z",
          "iopub.status.idle": "2024-01-29T11:44:10.526911Z",
          "shell.execute_reply.started": "2024-01-29T11:44:10.520407Z",
          "shell.execute_reply": "2024-01-29T11:44:10.525432Z"
        },
        "trusted": true,
        "id": "dyQc65Auh_35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.2 Get a sneak peek of data:\n",
        "The purpose of a sneak peek is to get a quick overview of the data and identify any potential problems or areas of interest."
      ],
      "metadata": {
        "id": "tNkzbOznh_35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the first 5 rows of the dataframe\n",
        "df.head(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:10.530673Z",
          "iopub.execute_input": "2024-01-29T11:44:10.531015Z",
          "iopub.status.idle": "2024-01-29T11:44:10.575514Z",
          "shell.execute_reply.started": "2024-01-29T11:44:10.530987Z",
          "shell.execute_reply": "2024-01-29T11:44:10.574208Z"
        },
        "trusted": true,
        "id": "UP_KKjgZh_35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2.3 Explore the Data"
      ],
      "metadata": {
        "id": "2zRqo882h_35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# exploring the datatype of each column\n",
        "df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:10.576815Z",
          "iopub.execute_input": "2024-01-29T11:44:10.57713Z",
          "iopub.status.idle": "2024-01-29T11:44:10.605719Z",
          "shell.execute_reply.started": "2024-01-29T11:44:10.577104Z",
          "shell.execute_reply": "2024-01-29T11:44:10.604438Z"
        },
        "trusted": true,
        "id": "aHXndrk5h_35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's have a look on the shape of the dataset:"
      ],
      "metadata": {
        "id": "L6JjyTQFh_35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data shpae\n",
        "df.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:10.60757Z",
          "iopub.execute_input": "2024-01-29T11:44:10.607962Z",
          "iopub.status.idle": "2024-01-29T11:44:10.614828Z",
          "shell.execute_reply.started": "2024-01-29T11:44:10.607933Z",
          "shell.execute_reply": "2024-01-29T11:44:10.613627Z"
        },
        "trusted": true,
        "id": "uE4xTbxch_36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets see the column name:"
      ],
      "metadata": {
        "id": "JSIHu5aCh_36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets see the column name\n",
        "df.columns"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:10.616435Z",
          "iopub.execute_input": "2024-01-29T11:44:10.616785Z",
          "iopub.status.idle": "2024-01-29T11:44:10.628432Z",
          "shell.execute_reply.started": "2024-01-29T11:44:10.616756Z",
          "shell.execute_reply": "2024-01-29T11:44:10.627296Z"
        },
        "trusted": true,
        "id": "3Uk0uRvLh_36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation Set 1:\n",
        "- There are 284807 rows and 31 columns in the dataset.\n",
        "- The data type of all columns is numeric (int and float).\n",
        "-  The columns in the datasets are:\n",
        "   - 'Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
        "      'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
        "      'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
        "      'Class'"
      ],
      "metadata": {
        "id": "KOpznhATh_36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "s2Epo3u3h_36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2.4 Descriptive Statistics:\n",
        "\n",
        "Descriptive statistics are a collection of quantitative measures that summarize and describe the main characteristics of a dataset."
      ],
      "metadata": {
        "id": "mWZ_PmjLh_36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary Statistics\n",
        "df.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:10.629854Z",
          "iopub.execute_input": "2024-01-29T11:44:10.630156Z",
          "iopub.status.idle": "2024-01-29T11:44:11.119538Z",
          "shell.execute_reply.started": "2024-01-29T11:44:10.630131Z",
          "shell.execute_reply": "2024-01-29T11:44:11.118391Z"
        },
        "trusted": true,
        "id": "JGVf1sA3h_36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "FqmSpqGRh_36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***3.3 Normalization:***"
      ],
      "metadata": {
        "id": "uYFAzRP1h_36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "normalizer = Normalizer(norm='l2')\n",
        "print(normalizer.fit_transform(df))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:11.12131Z",
          "iopub.execute_input": "2024-01-29T11:44:11.121677Z",
          "iopub.status.idle": "2024-01-29T11:44:11.266203Z",
          "shell.execute_reply.started": "2024-01-29T11:44:11.121646Z",
          "shell.execute_reply": "2024-01-29T11:44:11.265063Z"
        },
        "trusted": true,
        "id": "UXeaqHRih_36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***3.4 Lets deal with 'Class' column:***"
      ],
      "metadata": {
        "id": "Wy-RAAjRh_36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution of legitimate transactions & fraudulent transactions\n",
        "df['Class'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:11.271618Z",
          "iopub.execute_input": "2024-01-29T11:44:11.272343Z",
          "iopub.status.idle": "2024-01-29T11:44:11.2852Z",
          "shell.execute_reply.started": "2024-01-29T11:44:11.272299Z",
          "shell.execute_reply": "2024-01-29T11:44:11.283895Z"
        },
        "trusted": true,
        "id": "5mhH6iEQh_37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separating the data for analysis\n",
        "legit = df[df['Class'] == 0]\n",
        "fraud = df[df['Class'] == 1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:11.286619Z",
          "iopub.execute_input": "2024-01-29T11:44:11.287036Z",
          "iopub.status.idle": "2024-01-29T11:44:11.33217Z",
          "shell.execute_reply.started": "2024-01-29T11:44:11.287006Z",
          "shell.execute_reply": "2024-01-29T11:44:11.330918Z"
        },
        "trusted": true,
        "id": "iae3ZFh4h_37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# statistical measures of the legit data\n",
        "legit.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:11.333666Z",
          "iopub.execute_input": "2024-01-29T11:44:11.334403Z",
          "iopub.status.idle": "2024-01-29T11:44:11.820667Z",
          "shell.execute_reply.started": "2024-01-29T11:44:11.334361Z",
          "shell.execute_reply": "2024-01-29T11:44:11.819502Z"
        },
        "trusted": true,
        "id": "CcEIUh-Dh_37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "legit.Amount.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:11.822146Z",
          "iopub.execute_input": "2024-01-29T11:44:11.822516Z",
          "iopub.status.idle": "2024-01-29T11:44:11.84599Z",
          "shell.execute_reply.started": "2024-01-29T11:44:11.822466Z",
          "shell.execute_reply": "2024-01-29T11:44:11.844523Z"
        },
        "trusted": true,
        "id": "UI9lmH3th_37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# statistical measures of the fraud data\n",
        "fraud.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:11.847329Z",
          "iopub.execute_input": "2024-01-29T11:44:11.847705Z",
          "iopub.status.idle": "2024-01-29T11:44:11.959467Z",
          "shell.execute_reply.started": "2024-01-29T11:44:11.847673Z",
          "shell.execute_reply": "2024-01-29T11:44:11.958247Z"
        },
        "trusted": true,
        "id": "MkDYd31ih_37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fraud.Amount.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:11.960559Z",
          "iopub.execute_input": "2024-01-29T11:44:11.96089Z",
          "iopub.status.idle": "2024-01-29T11:44:11.972305Z",
          "shell.execute_reply.started": "2024-01-29T11:44:11.960864Z",
          "shell.execute_reply": "2024-01-29T11:44:11.97095Z"
        },
        "trusted": true,
        "id": "j0AmkoVQh_37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the values for both transactions\n",
        "df.groupby('Class').mean()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:11.973716Z",
          "iopub.execute_input": "2024-01-29T11:44:11.974069Z",
          "iopub.status.idle": "2024-01-29T11:44:12.090073Z",
          "shell.execute_reply.started": "2024-01-29T11:44:11.974041Z",
          "shell.execute_reply": "2024-01-29T11:44:12.088761Z"
        },
        "trusted": true,
        "id": "HYY4Ox8Zh_37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Under-Sampling (building sample dataset containing similar distribution of normal transactions and Fraudulent Transactions)\n",
        "legit_sample = legit.sample(n=492)\n",
        "# Concatenating two DataFrames\n",
        "new_df = pd.concat([legit_sample, fraud], axis=0)\n",
        "# Print first 5 rows of the new dataset\n",
        "new_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:12.091564Z",
          "iopub.execute_input": "2024-01-29T11:44:12.092003Z",
          "iopub.status.idle": "2024-01-29T11:44:12.140043Z",
          "shell.execute_reply.started": "2024-01-29T11:44:12.091971Z",
          "shell.execute_reply": "2024-01-29T11:44:12.138804Z"
        },
        "trusted": true,
        "id": "4terHIGUh_37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the distribution of the classes for the subsample dataset\n",
        "new_df['Class'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:12.141828Z",
          "iopub.execute_input": "2024-01-29T11:44:12.142284Z",
          "iopub.status.idle": "2024-01-29T11:44:12.152337Z",
          "shell.execute_reply.started": "2024-01-29T11:44:12.142242Z",
          "shell.execute_reply": "2024-01-29T11:44:12.151111Z"
        },
        "trusted": true,
        "id": "sH1Au7Hph_3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation Set 2:\n",
        "- I have performed the column of Class is divided into two parts: legit and fraud.\n",
        "- Dividing the class column into two parts resulted in data imbalance, to balance the imbalance I took the sample size of the legitimate part equal to that of the fraud part."
      ],
      "metadata": {
        "id": "W00oTqclh_3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "DwIDIu0_h_3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***4.0 Dealing with missing values:***"
      ],
      "metadata": {
        "id": "zQnNKXXyh_4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dealing with the missing values is one of the most important part of the data wrangling process, we must deal with the missing values in order to get the correct insights from the data."
      ],
      "metadata": {
        "id": "oWWmrBeeh_4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Missing Values\n",
        "df.isnull().sum().sort_values(ascending = False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:12.153892Z",
          "iopub.execute_input": "2024-01-29T11:44:12.15433Z",
          "iopub.status.idle": "2024-01-29T11:44:12.182438Z",
          "shell.execute_reply.started": "2024-01-29T11:44:12.154289Z",
          "shell.execute_reply": "2024-01-29T11:44:12.181167Z"
        },
        "trusted": true,
        "id": "hUC9G7E9h_4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone 1: There are no missing values in the dataset.\n",
        "---"
      ],
      "metadata": {
        "id": "k4GNILPlh_4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.0 Machine Learning:**"
      ],
      "metadata": {
        "id": "_VC9VQvsh_4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***5.1 Model Selection and Evaluation:***"
      ],
      "metadata": {
        "id": "a-aX10Dkh_4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lets make X and y in our Data:"
      ],
      "metadata": {
        "id": "cmPpQlmUh_4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:12.18416Z",
          "iopub.execute_input": "2024-01-29T11:44:12.184751Z",
          "iopub.status.idle": "2024-01-29T11:44:12.192467Z",
          "shell.execute_reply.started": "2024-01-29T11:44:12.184711Z",
          "shell.execute_reply": "2024-01-29T11:44:12.191284Z"
        },
        "trusted": true,
        "id": "3baCIqruh_4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into Features & Targets\n",
        "X = new_df.drop(columns='Class', axis=1)\n",
        "y = new_df['Class']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:12.194195Z",
          "iopub.execute_input": "2024-01-29T11:44:12.194654Z",
          "iopub.status.idle": "2024-01-29T11:44:12.205697Z",
          "shell.execute_reply.started": "2024-01-29T11:44:12.194612Z",
          "shell.execute_reply": "2024-01-29T11:44:12.20471Z"
        },
        "trusted": true,
        "id": "XJU_rzVvh_4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train_test_split the Data:"
      ],
      "metadata": {
        "id": "L-_aujq1h_4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into Training data & Testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:12.207103Z",
          "iopub.execute_input": "2024-01-29T11:44:12.207502Z",
          "iopub.status.idle": "2024-01-29T11:44:12.22222Z",
          "shell.execute_reply.started": "2024-01-29T11:44:12.20747Z",
          "shell.execute_reply": "2024-01-29T11:44:12.221295Z"
        },
        "trusted": true,
        "id": "FTCH3m4th_4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether the data is splitted in 80:20 ratio\n",
        "print(X.shape, X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:12.223699Z",
          "iopub.execute_input": "2024-01-29T11:44:12.224121Z",
          "iopub.status.idle": "2024-01-29T11:44:12.236132Z",
          "shell.execute_reply.started": "2024-01-29T11:44:12.224081Z",
          "shell.execute_reply": "2024-01-29T11:44:12.235003Z"
        },
        "trusted": true,
        "id": "FXavyBn6h_4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Call, Train and Predict the Model:"
      ],
      "metadata": {
        "id": "t7LSURKHh_4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the Model\n",
        "model = RandomForestClassifier(random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:12.237435Z",
          "iopub.execute_input": "2024-01-29T11:44:12.237798Z",
          "iopub.status.idle": "2024-01-29T11:44:12.248054Z",
          "shell.execute_reply.started": "2024-01-29T11:44:12.23776Z",
          "shell.execute_reply": "2024-01-29T11:44:12.246914Z"
        },
        "trusted": true,
        "id": "BuNZZkIlh_4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# Create a pipeline for each model\n",
        "pipeline = Pipeline([\n",
        "    ('model', model)\n",
        "    ])\n",
        "\n",
        "# Perform cross-validation\n",
        "scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
        "\n",
        "# Calculate mean accuracy\n",
        "mean_accuracy = scores.mean()\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model:\", RandomForestClassifier())\n",
        "print(\"Cross-validation Accuracy:\", mean_accuracy)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "print('Recall Score: ', recall_score(y_test, y_pred))\n",
        "print('Precision Score: ', precision_score(y_test, y_pred))\n",
        "print('F1 Score: ', f1_score(y_test, y_pred))\n",
        "\n",
        "best_model = pipeline\n",
        "\n",
        "# save the best model\n",
        "import pickle\n",
        "pickle.dump(best_model, open('iris_model.dot', 'wb'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:12.249783Z",
          "iopub.execute_input": "2024-01-29T11:44:12.250514Z",
          "iopub.status.idle": "2024-01-29T11:44:14.574332Z",
          "shell.execute_reply.started": "2024-01-29T11:44:12.250471Z",
          "shell.execute_reply": "2024-01-29T11:44:14.57316Z"
        },
        "trusted": true,
        "id": "_RKYd56Sh_4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visulalizing the confusion matrix\n",
        "LABELS = ['Normal', 'Fraud']\n",
        "from sklearn.metrics import confusion_matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize =(12, 12))\n",
        "sns.heatmap(conf_matrix, xticklabels = LABELS, yticklabels = LABELS, annot = True, fmt =\"d\");\n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.ylabel('True class')\n",
        "plt.xlabel('Predicted class')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:14.575943Z",
          "iopub.execute_input": "2024-01-29T11:44:14.576292Z",
          "iopub.status.idle": "2024-01-29T11:44:14.928739Z",
          "shell.execute_reply.started": "2024-01-29T11:44:14.576263Z",
          "shell.execute_reply": "2024-01-29T11:44:14.927541Z"
        },
        "trusted": true,
        "id": "Gu1skO_jh_4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Feature Engineering:***"
      ],
      "metadata": {
        "id": "tBqc7AgCh_4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = new_df.copy()\n",
        "df_train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:14.93053Z",
          "iopub.execute_input": "2024-01-29T11:44:14.931226Z",
          "iopub.status.idle": "2024-01-29T11:44:14.965987Z",
          "shell.execute_reply.started": "2024-01-29T11:44:14.931183Z",
          "shell.execute_reply": "2024-01-29T11:44:14.964853Z"
        },
        "trusted": true,
        "id": "wLEjhZ2Sh_4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The function display_feature_importance takes a machine learning model (model) as input and performs feature importance analysis.\n",
        "def display_feature_importance(model,percentage ,top_n=34, plot=False):\n",
        "    # X and y\n",
        "    X = df_train.drop('Class',axis=1)\n",
        "    y = df_train['Class']\n",
        "\n",
        "    #The model is fitted using the features (X) and the target variable (y), and then the feature importances are calculated.\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Get feature importance\n",
        "    feature_importance = model.feature_importances_\n",
        "    feature_names = X.columns\n",
        "\n",
        "    # Create a DataFrame for better visualization\n",
        "    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
        "\n",
        "    # Sort features by importance\n",
        "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    # A threshold is calculated based on a specified percentage of the top feature importance.\n",
        "    #  Features with importance above this threshold are selected.\n",
        "    threshold = percentage / 100 * feature_importance_df.iloc[0]['Importance']\n",
        "\n",
        "    # Select features that meet the threshold\n",
        "    selected_features = feature_importance_df[feature_importance_df['Importance'] >= threshold]['Feature'].tolist()\n",
        "\n",
        "    #Print Selected Feature\n",
        "    print(\"Selected Features by {} \\n \\n at threshold {}%; {}\".format(model , percentage,selected_features))\n",
        "\n",
        "    if plot==True:\n",
        "        # Set seaborn color palette to \"viridis\"\n",
        "        sns.set(style=\"whitegrid\", palette=\"viridis\")\n",
        "\n",
        "        # Display or plot the top features\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(top_n))\n",
        "        plt.title('Feature Importance for {}'.format(type(model).__name__))\n",
        "        plt.show()\n",
        "\n",
        "    # Add 'Exited' to the list of selected features\n",
        "    selected_features.append('Class')\n",
        "\n",
        "    return selected_features"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:14.971484Z",
          "iopub.execute_input": "2024-01-29T11:44:14.971832Z",
          "iopub.status.idle": "2024-01-29T11:44:14.983431Z",
          "shell.execute_reply.started": "2024-01-29T11:44:14.971801Z",
          "shell.execute_reply": "2024-01-29T11:44:14.982272Z"
        },
        "trusted": true,
        "id": "hILZyduEh_4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store selected features for each model and trial percentage\n",
        "selected_features_xgb = []\n",
        "\n",
        "# Initilize AUC List\n",
        "auc_scores = []\n",
        "\n",
        "# List of trial percentages\n",
        "trial_percentages = [3, 5, 10, 20, 40]\n",
        "\n",
        "# Loop over each trial percentage\n",
        "for percentage in trial_percentages:\n",
        "        # Get selected features for each model\n",
        "        xgb_selected_features = display_feature_importance(XGBClassifier(random_state=42), percentage=percentage)\n",
        "\n",
        "        # Append selected features to the respective lists\n",
        "        selected_features_xgb.append(xgb_selected_features)\n",
        "\n",
        "        # X and y\n",
        "        X = df_train.drop('Class',axis=1)\n",
        "        y = df_train['Class']\n",
        "\n",
        "        # Split the data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "        # Fit models on training data\n",
        "        xgb_model = XGBClassifier()\n",
        "        xgb_model.fit(X_train[[feature for feature in xgb_selected_features if feature != 'Class']], y_train, verbose=0)\n",
        "\n",
        "        # Predict probabilities on the test set\n",
        "        xgb_pred_proba = xgb_model.predict_proba(X_test[[feature for feature in xgb_selected_features if feature != 'Class']])[:, 1]\n",
        "\n",
        "        # Calculate AUC scores and append to the list\n",
        "        from sklearn.metrics import roc_auc_score\n",
        "\n",
        "        auc_xgb = roc_auc_score(y_test, xgb_pred_proba)\n",
        "        auc_scores.append((auc_xgb,percentage))\n",
        "\n",
        "        # Sorted AUC\n",
        "        sorted_auc = sorted(auc_scores, reverse=True)\n",
        "\n",
        "# Print Each AUC with Percentage\n",
        "for score , percentage in sorted_auc :\n",
        "        print(f'The AUC for {type(xgb_model).__name__ , } \\n with {percentage}% of top features is {score:.4f}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:14.984761Z",
          "iopub.execute_input": "2024-01-29T11:44:14.985081Z",
          "iopub.status.idle": "2024-01-29T11:44:16.102713Z",
          "shell.execute_reply.started": "2024-01-29T11:44:14.985054Z",
          "shell.execute_reply": "2024-01-29T11:44:16.10177Z"
        },
        "trusted": true,
        "id": "ZpnsZdIuh_4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impottant Feature Extract Using XGb\n",
        "imp_fea = ['V14', 'V10', 'V4', 'V7', 'V21', 'V8', 'V20', 'V3', 'V5', 'V11', 'V12', 'V26', 'V17','Class']\n",
        "df_train = df_train[imp_fea]\n",
        "df_train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:16.107141Z",
          "iopub.execute_input": "2024-01-29T11:44:16.10784Z",
          "iopub.status.idle": "2024-01-29T11:44:16.137308Z",
          "shell.execute_reply.started": "2024-01-29T11:44:16.107804Z",
          "shell.execute_reply": "2024-01-29T11:44:16.136464Z"
        },
        "trusted": true,
        "id": "RJh4SjRxh_4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:16.138927Z",
          "iopub.execute_input": "2024-01-29T11:44:16.139916Z",
          "iopub.status.idle": "2024-01-29T11:44:16.147357Z",
          "shell.execute_reply.started": "2024-01-29T11:44:16.139875Z",
          "shell.execute_reply": "2024-01-29T11:44:16.146232Z"
        },
        "trusted": true,
        "id": "4I0Ypnx3h_4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Important Feature of  Dataset Train RF and XGB with Hyperparameter Tuning:*"
      ],
      "metadata": {
        "id": "NRhogGaPh_4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **RF**"
      ],
      "metadata": {
        "id": "FgFn_OQ5h_4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_random_forest(data, target):\n",
        "    # Dictionary to store LabelEncoders for each categorical column\n",
        "    label_encoders = {}\n",
        "\n",
        "    # split the data into X and y\n",
        "    X = data.drop(target, axis=1)\n",
        "    y = data[target]\n",
        "\n",
        "    # split the data into train and test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "    # # Scaling Data\n",
        "    # scaler = MinMaxScaler()\n",
        "    # X_train = scaler.fit_transform(X_train)\n",
        "    # X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Define the Random Forest model\n",
        "    rf_model = RandomForestClassifier(random_state=0,class_weight='balanced')\n",
        "\n",
        "    # Define hyperparameters for tuning\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "\n",
        "    # Perform GridSearchCV for hyperparameter tuning\n",
        "    grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best model and parameters\n",
        "    best_rf_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Print the best hyperparameters\n",
        "    print('Best Hyperparameters:')\n",
        "    print(best_params)\n",
        "\n",
        "    # Train the model on the full training set\n",
        "    best_rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    y_pred_rf = best_rf_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "    precision = precision_score(y_test, y_pred_rf)\n",
        "    recall = recall_score(y_test, y_pred_rf)\n",
        "\n",
        "    print(f'Accuracy on Test Set: {accuracy:.2f}')\n",
        "    print(f'Precision on Test Set: {precision:.2f}')\n",
        "    print(f'Recall on Test Set: {recall:.2f}')\n",
        "\n",
        "    #visulalizing the confusion matrix\n",
        "    LABELS = ['Normal', 'Fraud']\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred_rf)\n",
        "    plt.figure(figsize =(12, 12))\n",
        "    sns.heatmap(conf_matrix, xticklabels = LABELS, yticklabels = LABELS, annot = True, fmt =\"d\");\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.ylabel('True class')\n",
        "    plt.xlabel('Predicted class')\n",
        "    plt.show()\n",
        "\n",
        "    return best_rf_model, best_params, accuracy\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:16.149001Z",
          "iopub.execute_input": "2024-01-29T11:44:16.150172Z",
          "iopub.status.idle": "2024-01-29T11:44:16.166076Z",
          "shell.execute_reply.started": "2024-01-29T11:44:16.150131Z",
          "shell.execute_reply": "2024-01-29T11:44:16.164914Z"
        },
        "trusted": true,
        "id": "hV8bxdzTh_4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_random_forest(df_train,'Class')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:44:16.16742Z",
          "iopub.execute_input": "2024-01-29T11:44:16.167882Z",
          "iopub.status.idle": "2024-01-29T11:46:17.290614Z",
          "shell.execute_reply.started": "2024-01-29T11:44:16.167851Z",
          "shell.execute_reply": "2024-01-29T11:46:17.289338Z"
        },
        "trusted": true,
        "id": "xJXrfY-rh_4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **XGB**"
      ],
      "metadata": {
        "id": "MwxAhYySh_4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_xgb_classifier(data, target):\n",
        "    # split the data into X and y\n",
        "    X = data.drop(target, axis=1)\n",
        "    y = data[target]\n",
        "\n",
        "    # split the data into train and test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "\n",
        "    # # Scaling Data\n",
        "    # scaler = MinMaxScaler()\n",
        "    # X_train = scaler.fit_transform(X_train)\n",
        "    # X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Define the XGBClassifier model\n",
        "    xgb_model = XGBClassifier(random_state=0)\n",
        "\n",
        "    # Define hyperparameters for tuning\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0],\n",
        "        'gamma': [0, 1, 2]\n",
        "    }\n",
        "\n",
        "    # Perform GridSearchCV for hyperparameter tuning\n",
        "    grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best model and parameters\n",
        "    best_xgb_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Print the best hyperparameters\n",
        "    print('Best Hyperparameters:')\n",
        "    print(best_params)\n",
        "\n",
        "    # Train the model on the full training set\n",
        "    best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    y_pred_xgb = best_xgb_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred_xgb)\n",
        "    precision = precision_score(y_test, y_pred_xgb)\n",
        "    recall = recall_score(y_test, y_pred_xgb)\n",
        "\n",
        "    print(f'Accuracy on Test Set: {accuracy:.2f}')\n",
        "    print(f'Precision on Test Set: {precision:.2f}')\n",
        "    print(f'Recall on Test Set: {recall:.2f}')\n",
        "\n",
        "    #visulalizing the confusion matrix\n",
        "    LABELS = ['Normal', 'Fraud']\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred_xgb)\n",
        "    plt.figure(figsize =(12, 12))\n",
        "    sns.heatmap(conf_matrix, xticklabels = LABELS, yticklabels = LABELS, annot = True, fmt =\"d\");\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.ylabel('True class')\n",
        "    plt.xlabel('Predicted class')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    return best_xgb_model, best_params"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:46:17.292257Z",
          "iopub.execute_input": "2024-01-29T11:46:17.29263Z",
          "iopub.status.idle": "2024-01-29T11:46:17.307091Z",
          "shell.execute_reply.started": "2024-01-29T11:46:17.292601Z",
          "shell.execute_reply": "2024-01-29T11:46:17.305866Z"
        },
        "trusted": true,
        "id": "CMrFvGqJh_4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_xgb_classifier(df_train,'Class')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:46:17.30848Z",
          "iopub.execute_input": "2024-01-29T11:46:17.308806Z",
          "iopub.status.idle": "2024-01-29T11:48:37.796999Z",
          "shell.execute_reply.started": "2024-01-29T11:46:17.308779Z",
          "shell.execute_reply": "2024-01-29T11:48:37.795901Z"
        },
        "trusted": true,
        "id": "OW9UQQNVh_4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selcting Best and Highest Accuracy from Above trained Models\n",
        "# XGb Model Classifier\n",
        "# Random Forest\n",
        "models = ['XGB Classifier', 'RandomForestClassifier']\n",
        "accuracy_scores = [accuracy, accuracy]\n",
        "\n",
        "# Find the index of the maximum accuracy\n",
        "best_accuracy_index = accuracy_scores.index(max(accuracy_scores))\n",
        "\n",
        "# Print the best model for accuracy\n",
        "print(f'Best Accuracy: {accuracy_scores[best_accuracy_index]:.2f} with Model: {models[best_accuracy_index]}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-29T11:48:37.798827Z",
          "iopub.execute_input": "2024-01-29T11:48:37.799378Z",
          "iopub.status.idle": "2024-01-29T11:48:37.805663Z",
          "shell.execute_reply.started": "2024-01-29T11:48:37.799347Z",
          "shell.execute_reply": "2024-01-29T11:48:37.804687Z"
        },
        "trusted": true,
        "id": "riPbDRdlh_4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation Set 3:\n",
        "- I used two models: XGB Classifier and RandomForest Classifier.\n",
        "- I selected the best model, which is XGB Classifier.\n",
        "- The best accuracy is 0.93 with model: XGB Classifier."
      ],
      "metadata": {
        "id": "PIE4-K6Vh_4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "96VZV0h_h_4E"
      }
    }
  ]
}